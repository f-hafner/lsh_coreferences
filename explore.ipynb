{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-sensitive hashing for detecting coreferences\n",
    "\n",
    "Problem: We want to find mentions that cold refer to the same entity. Currently we compare each mention with each other, and if mention 1 contains mention 0 as a word, then we consider mention 0 and mention 1 potential coreferences. This approach does not scale well with the number of mentions.\n",
    "\n",
    "Solution: try out how to use locality-sensitive hashing to reduce the number of comparisons to make. While there is optimized software available to do this, I think that a good start is a solution without external dependencies: no need to check compatibility with other requirements, and data are already preprocessed which should make the task computationally simple. \n",
    "\n",
    "How does LSH work?\n",
    "1. Shingling: convert text to sparse vectors. I will start with shingling at the word level and think about alternatives later.\n",
    "    - One-hot encoding\n",
    "    - Define vocabulary of all shingles\n",
    "2. MinHashing: create signatures based on randomly reordering the vocabulary and recording the first shingle that is in mention $i$.\n",
    "3. Cut the signature into bands and assign--using a hash function--all mentions to buckets. \n",
    "    - More bands means larger buckets\n",
    "    - Need to use the same function for the same band $j$ for all mentions. Can use different functions for different bands, but not required. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the hashing\n",
    "The approach with min hashing is not feasible for high-dimensional data sets because it is expensive to randomly shuffle large arrays and/or store large binary arrays in memory. But there are alternatives. One is with random projections, as discussed in the wikipedia page, in [this video](https://www.youtube.com/watch?v=Arni-zkqMBA) and in [this book](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf) (section 3.7.2 Random Hyperplanes and the Cosine Distance).\n",
    "\n",
    "What is the idea?\n",
    "- Use the cosine similarity between two binary vectors with the cosine distance\n",
    "- Suppose we pick a hyperplane through the origin. We do so by choosing a vector $v$ that is perpendicular to the hyperplane. \n",
    "    - Then, either two vectors $x$ and $y$ lie on the same side of the hyperplane or they do not. \n",
    "- Then, choose a random vector $v_f$. \n",
    "    - we can build a hashing function $f$ that assigns the same value to $x$ and $y$ if they lie on the same side of the hyperplane that $v$ is perpendicular to (the dot product $x.v$ and $y.v$ will be informative about this.)\n",
    "    - (the angle $\\theta$ between $x$ and $y$ will determine the probability that $x$ and $y$ are on the same side of a given hyperplane; see book for intuition)\n",
    "    - the family $F$ of functions defined by vectors $v_f$ is locality-sensitive for the cosine distance. And this is very similar to the Jaccard-distance family, up to some scaling factor. \n",
    "- We can then build sketches (see the code [here](https://github.com/brandonrobertz/SparseLSH/blob/11f381560a94c8d74af55b3db5e8db1bbddfc212/sparselsh/lsh.py#L140)) by using random vector whose elements are either +1 or -1.\n",
    "    - consider a random vector $v$ where $v_i \\in \\{-1, 1\\} \\forall i$.\n",
    "    - we calculate the dot product for a random vector $v$ and vector $x$.\n",
    "    - the dot product is the difference between the sum of all elements $x[i]$ for $i: v[i] = 1$, and the sum of all elements $x[i]$ for $i: v[i] = -1$. \n",
    "    - We repeat this for multiple vectors $v$ and store whether the dot product is positive or negative (again by $+1$ or $-1$). Since a dot product of $0$ is unlikely, we can handle such cases arbitrarily. \n",
    "    - The result of this is called a **sketch** (which is the same as a signature, see [p. 49 here](https://web.stanford.edu/class/cs246/slides/04-lsh_theory.pdf))\n",
    "- I do not understand the example 3.22: does it imply that we have to take a large number of random vectors? what is the \n",
    "- p. 99: cosine distance makes sense in spaces [...] where points are vectors with integer components or Boolean components -- thus, we can use it here. \n",
    "\n",
    "Now I am not sure how SparseLSH implements the whole thing. They have different distance options, but the hashing is, as far as I understand, the same for all options. What is the theory behind this? Can I just use this hashing instead of my hashing, and then continue with the signature as before (which is essentially the Jaccard distance??)\n",
    "    - or is this perhaps what is discussed [here](https://ai.stackexchange.com/questions/37199/clustering-by-using-locality-sensitive-hashing-after-random-projection)?\n",
    "    - in fact, the slides from Stanford seem to imply that using cosine distance for LSH in the same way min-hashing was used for the Jaccard distance.\n",
    "\n",
    "The output from the random projections is again a (denser) vector of -1s and +1s. Because this carries much less information than the real-valued vectors from the minhashing, the banding technique does not work--too many items would have the same band. So, what does the SparseLSH apply then? What do they write in the book? What does wikipedia say?\n",
    "- see [here, p. 58](https://web.stanford.edu/class/cs246/slides/04-lsh_theory.pdf): they apply the bands technique to the Euclidean distance\n",
    "\n",
    "[continue in pdf]\n",
    "\n",
    "section 3.6: family of locality-senstive functions\n",
    "- minhash function is one family of locality-sensitive functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lessons I learned\n",
    "- It is important to use the binary vectors. If using the min-hashed vectors, longer words are closer to longer ones (and shorter to shorter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_coreferences import load_coreferences\n",
    "# import lsh \n",
    "import REL # install with pip install -e ../REL/. (or pip install -e ../REL/.[develop])\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from REL import lsh \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps, 16/01/23\n",
    "- [ ] tidy the lsh class\n",
    "    - remove comments etc\n",
    "    - fix TODOs\n",
    "    - rename the class - it is not MinHash anymore\n",
    "- [ ] rerun all tests. does profiling work now? why not?\n",
    "- [ ] better understand time complexity\n",
    "    - how does it depend on the parameters? -- the youtube video is helpful\n",
    "    - automatically adjust the inputs to LSH as a function of the number of mentions?\n",
    "- [ ] improve ROC curves\n",
    "    - they look a bit funny, are they correct?\n",
    "    - what does precision measure? it just means that we retrieve many more items in a group right? can we display the implied group size? this could be informative.\n",
    "    - repeat the exercises for all mentions, no only coreferring mentions? \n",
    "- [ ] try to make faster\n",
    "    - still quite some overhead of with_coref function (but lower than before)\n",
    "    - is there a way to improve upon it?\n",
    "    - profiling output:\n",
    "        1. [x] encode_binary - try sklearn? but another dependency(?) \n",
    "        2. [ ] get_candidates. Now this is the bottleneck\n",
    "        3. [ ] (idx_unique_multidim) -- but at first sight it seems unavoidable and already optimised\n",
    "            - the function idx_unique_multidim itself is fast. but it is called for each band. can it be vectorized. \n",
    "        - also: are there better ways to choose the vectors (to make sure that we have many that are sufficiently different from each other?)\n",
    "    - add external options for benchmarking -- benchmark for efficiency and effectiveness\n",
    "        - faiss \n",
    "        - datasketch\n",
    "- [ ] fix script with evaluations from Erik (see PR on github)\n",
    "- [ ] scaling of mentions: use all data sets (current default is 50). go back to less scaling? \n",
    "    - problem with document_nstack \"1208testb_300\". why?\n",
    "    - I am not sure anymore how good this stacking is for testing lsh: by stacking the same mentions, the number of comparisons mechanically increases. But this seems to be a problem only for lsh, ie, does it produce an lower bound for the efficiency gain of going from \"all\" to \"lsh\"?\n",
    "- [X] check what is going on for msmarco: size of the candidate groups?\n",
    "    - print this out at the end of the hashing for information\n",
    "    - maybe store it also as another output? \n",
    "- [ ] inputs to class should be band length and number of bands; then signature can be calculated directly for any inputs\n",
    "- [x] try out an option with even longer signature and longer bands? would that perhaps increase precision without lowering recall? \n",
    "- [ ] speed up grouping? avoid the quadratic time complexity there? -- check the sorted list approach\n",
    "- [ ] scaling of mentions: set limit of number of mentions, for instance around as many as in 1208testb_100.\n",
    "- [ ] strictly speaking the parameters for LSH should be chosen based on the training data set, but currently they are chosen with the test data set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mentions = load_coreferences(drop_duplicates=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = {i: m for i, m in enumerate(raw_mentions)}\n",
    "# stack them on top of each other \n",
    "\n",
    "mentions_scaled = copy.copy(mentions)\n",
    "\n",
    "idx = len(mentions_scaled)\n",
    "scaling_factor = 5\n",
    "for i in range(1, scaling_factor):\n",
    "    for idx_old in mentions.keys():\n",
    "        m = mentions[idx_old]\n",
    "        mentions_scaled[idx] = m \n",
    "        idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to integrate with REL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.007684469223022461 seconds for 30 mentions\n",
      "average, min, max cluster size: 6.07, 0, 13\n"
     ]
    }
   ],
   "source": [
    "# out = {i: {\"shingles\": lsh.k_shingle(m, 4)} for i, m in zip(range(len(mentions_rel), mentions)) }\n",
    "# mentions_rel\n",
    "mentions_rel = [\n",
    "    'German', 'British', 'Brussels', 'European Commission', 'German',\n",
    "    'British', 'Germany', 'European Union', 'Britain', 'Commission', \n",
    "    'European Union', 'Franz Fischler', 'Britain', 'France', 'BSE', \n",
    "    'Spanish', 'Loyola de Palacio', 'France', 'Britain', 'BSE', 'British', 'German', \n",
    "    'British', 'Europe', 'Germany', 'Bonn', 'British', 'Germany', 'Britain', 'British'\n",
    "]\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=mentions_rel, shingle_size=2, signature_size=900, band_length=15)\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to deal with input where only one item is repeated? -- put as a test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1}, {0, 1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lsh \n",
    "import numpy as np\n",
    "\n",
    "test_mentions = ['EEC', 'EEC'] # this fails. ['German', 'German'] does not. neither does ['EEC', 'EEC', 'German']\n",
    "test_mentions = ['EEC', 'ABC'] # this also fails. ['EEC', 'ABCde'] does not. \n",
    "# the problem is that when there is no mention of at least the inputted shingle size, the binary vectors cannot be calculated\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=test_mentions, shingle_size=4, signature_size=300, band_length=2)\n",
    "\n",
    "mylsh._build_vocab()\n",
    "mylsh.encode_binary(to_numpy=True)\n",
    "\n",
    "mylsh.vectors\n",
    "\n",
    "mylsh.vectors.shape[1] # this should not be 0\n",
    "mylsh.vectors.shape\n",
    "# mylsh.make_signature()\n",
    "mylsh.cluster()\n",
    "mylsh.candidates\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "541f9b2bd2e07a99fbe19519917a2a847d7149d43292ee48064a4037dd0b7699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
