{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-sensitive hashing for detecting coreferences\n",
    "\n",
    "Problem: We want to find mentions that cold refer to the same entity. Currently we compare each mention with each other, and if mention 1 contains mention 0 as a word, then we consider mention 0 and mention 1 potential coreferences. This approach does not scale well with the number of mentions.\n",
    "\n",
    "Solution: try out how to use locality-sensitive hashing to reduce the number of comparisons to make. While there is optimized software available to do this, I think that a good start is a solution without external dependencies: no need to check compatibility with other requirements, and data are already preprocessed which should make the task computationally simple. \n",
    "\n",
    "How does LSH work?\n",
    "1. Shingling: convert text to sparse vectors. I will start with shingling at the word level and think about alternatives later.\n",
    "    - One-hot encoding\n",
    "    - Define vocabulary of all shingles\n",
    "2. MinHashing: create signatures based on randomly reordering the vocabulary and recording the first shingle that is in mention $i$.\n",
    "3. Cut the signature into bands and assign--using a hash function--all mentions to buckets. \n",
    "    - More bands means larger buckets\n",
    "    - Need to use the same function for the same band $j$ for all mentions. Can use different functions for different bands, but not required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_coreferences import load_coreferences\n",
    "import lsh \n",
    "import copy\n",
    "\n",
    "import faiss \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mentions = load_coreferences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = {i: m for i, m in enumerate(raw_mentions)}\n",
    "# stack them on top of each other \n",
    "\n",
    "mentions_scaled = copy.copy(mentions)\n",
    "\n",
    "idx = len(mentions_scaled)\n",
    "scaling_factor = 20\n",
    "for i in range(1, scaling_factor):\n",
    "    for idx_old in mentions.keys():\n",
    "        m = mentions[idx_old]\n",
    "        mentions_scaled[idx] = m \n",
    "        idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.550947904586792 seconds to classify 3480 mentions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mylsh = lsh.LSHBitSampling(mentions=mentions_scaled, shingle_size=4)\n",
    "\n",
    "mylsh.faiss_neighbors(k=10, nbits=100)\n",
    "neighbors = mylsh.neighbors_to_dict(mentions_scaled)\n",
    "mylsh.summarise() # note: 50% of the time here is taken for extracting the neighbors! can this be made faster? ie with numpy?\n",
    "\n",
    "# note: this approach uses quite some memory because it stores the full vectors and shingles.. change??\n",
    "# here we see the problem -- there are 20 duplicates of any mention, but the 10 nearest neighbors are all in the same bucket\n",
    "    # how to avoid? we do not know the number of duplicates beforehand... count? somehow create unique mentions to avoid this? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes \n",
    "- It is important to use the binary vectors. If using the min-hashed vectors, longer words are closer to longer ones (and shorter to shorter)\n",
    "- higher k -> higher recall\n",
    "    - low k may not capture the actual coreferences. ie, for k=4, it missed them for \"eagles\", \"rosati\", \"belo\"\n",
    "    - using k=10 fixes the problems for \"eagles\" and \"belo\" but not for \"rosati\"\n",
    "\n",
    "\n",
    "Next steps\n",
    "- scaling -- seems very good \n",
    "- precision/recall of whole pipeline (including the search for coreferences)\n",
    "- how easy is it to integrate FAISS into REL?\n",
    "- a problem could be when there are a lot of mentions that refer to the same (single) mention. ie, 10 times \"jimi\" and once \"jimi hendrix\". \n",
    "    - I think we would miss \"jimi hendrix\" for k=10\n",
    "    - Solutions: larger k? -> quadratic cost again \n",
    "    - Restrict the hamming distance of the neighbors, ie forbid them to be in the same bucket (this would also omit the own element already I think)\n",
    "    - do some more testing for this \n",
    "    - or just use the unique mentions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with MinHashing (does not scale well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 49.10193204879761 seconds for 3480 mentions\n",
      "average, min, max cluster size: 78.08, 39, 259\n"
     ]
    }
   ],
   "source": [
    "len(mentions_scaled)\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=mentions_scaled, shingle_size=3, signature_size=200, n_buckets=2)\n",
    "\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()\n",
    "\n",
    "# adjust the sizing according to the rules so that we have log-time complexity!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define comparison groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps\n",
    "- how does it scale?\n",
    "    - try it out with the prototype?\n",
    "- optimize for the use case (number of hashes, size of subvectors)\n",
    "- numpy? use external library? \n",
    "    - https://github.com/spotify/annoy\n",
    "    - https://faiss.ai/\n",
    "- look at typical coreferences: use other examples for the above! \n",
    "- should we change the definition of what is a coreference? \n",
    "- add test data, ie precision/recall for the input data? (after proper classification)\n",
    "    - also add some mentions that are not coreferences (why not all?)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuff: trying out FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylsh = lsh.LSH(mentions=mentions_scaled, shingle_size=3, signature_size=200, n_buckets=2)\n",
    "\n",
    "mylsh._build_vocab()\n",
    "mylsh._one_hot_encode()\n",
    "mylsh._min_hash()\n",
    "mylsh._make_bands()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mylsh.mentions[0][\"signature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = 64                           # dimension -- number of columns\n",
    "nb = 100000                      # database size -- number of vectors (=number of rows)\n",
    "nq = 10000                       # nb of queries\n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "xb[:, 0] += np.arange(nb) / 1000.\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "xq[:, 0] += np.arange(nq) / 1000.\n",
    "\n",
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "print(index.is_trained)\n",
    "index.add(xb)                  # add vectors to the index\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(type(faiss.IndexFlatL2))\n",
    "sum(xb[:10, :] != xb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  393  363   78]\n",
      " [   1  555  277  364]\n",
      " [   2  304  101   13]\n",
      " [   3  173   18  182]\n",
      " [   4  288  370  531]\n",
      " [   5  936  817 1316]\n",
      " [   6   35  142 1021]\n",
      " [   7  175  415  673]\n",
      " [   8   18  434   84]\n",
      " [   9 1076  622  801]]\n",
      "[[0.        7.1751738 7.20763   7.2511625]\n",
      " [0.        6.3235645 6.684581  6.799946 ]\n",
      " [0.        5.7964087 6.391736  7.2815123]\n",
      " [0.        7.2779055 7.527987  7.6628466]\n",
      " [0.        6.7638035 7.2951202 7.3688145]\n",
      " [0.        6.8761454 7.136672  7.2297354]\n",
      " [0.        6.765587  7.6871905 7.940848 ]\n",
      " [0.        6.2155056 6.7339525 6.762247 ]\n",
      " [0.        7.0762296 7.300755  7.542604 ]\n",
      " [0.        6.2345862 6.455964  6.6127834]]\n"
     ]
    }
   ],
   "source": [
    "k = 4                     # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:10], k) \n",
    "print(I)\n",
    "print(D)\n",
    "\n",
    "# xb[:10] is the query vector: the first 10 rows. maybe more intuitive: xb[:10, :]\n",
    "    # ie, for the first 10 vectors in the database, we look for the closest k neighbors in itself (the whole database)\n",
    "# what are the outputs?\n",
    "    # D: distance matrix nq-by-k: number of rows = size of query, number of columns = number of neighbors\n",
    "        # ordered by increasing distance, ie D[:, 0] is the distance to the first nearest neighbor, D[:, 1] is the distance to the second nearest neighbor\n",
    "        # the distance is defined by the index type (L2 norm here)\n",
    "    # I: row i contains the IDs of the neighbors of query vector i, sorted by inreasing distance\n",
    "        # ie, below, the first nearest neighbor to vector 0 is itself. the next nearest neighbor is vector 393\n",
    "        # the first nearest neighbor to vector 1 is also itself. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 393 363  78]\n",
      " [  1 555 277 364]\n",
      " [  2 304 101  13]\n",
      " [  3 173  18 182]\n",
      " [  4 288 370 531]]\n",
      "[[0.        7.1751738 7.20763   7.2511625]\n",
      " [0.        6.3235645 6.684581  6.799946 ]\n",
      " [0.        5.7964087 6.391736  7.2815123]\n",
      " [0.        7.2779055 7.527987  7.6628466]\n",
      " [0.        6.7638035 7.2951202 7.3688145]]\n",
      "[[ 381  207  210  477]\n",
      " [ 526  911  142   72]\n",
      " [ 838  527 1290  425]\n",
      " [ 196  184  164  359]\n",
      " [ 526  377  120  425]]\n",
      "[[ 9900 10500  9309  9831]\n",
      " [11055 10895 10812 11321]\n",
      " [11353 11103 10164  9787]\n",
      " [10571 10664 10632  9638]\n",
      " [ 9628  9554 10036  9582]]\n"
     ]
    }
   ],
   "source": [
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:5], k) # sanity check\n",
    "print(I)\n",
    "print(D)\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print(I[-5:])                  # neighbors of the 5 last queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try alternative distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "nbits = 15 # is this the length of the signature?? but the signature is already in the dense vector? \n",
    "index = faiss.IndexLSH(d, nbits)   # build the index\n",
    "print(index.is_trained)\n",
    "index.add(xb)                 # add vectors to the index\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  86  73  29]\n",
      " [  1 215 239 199]\n",
      " [  2 528 282  69]\n",
      " [  3 754 415  61]\n",
      " [  4 178 204   8]]\n",
      "[[0. 1. 1. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 1. 1. 2.]\n",
      " [0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:5], k) # sanity check -- for short nbits, it may not even assign itself as the first closest neighbor\n",
    "print(I) # the nearest neighbors change when changing the number of bits (and therefore buckets)\n",
    "print(D) # the distance here indicates whether the comparison records are in the same bucket or not\n",
    "    # what do the integers mean here? 0 = same bucket, 1=? 2=?\n",
    "    # the distance is the hamming distance? -- taken from the signature? ie, signature more different if distance is larger? \n",
    "\n",
    "\n",
    "# so nbits refers to the length of the subvector that we cut for comparing hashes? this would meen that nbits cannot be larger than the dimensionality?\n",
    "    # i am still not sure how to go from the theory/simple prototype to the faiss library \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "174\n",
      "[[  0 139  19 ...  24 105  16]\n",
      " [  1 152  25 ... 134  31  97]\n",
      " [  2  17 142 ... 112  83   3]\n",
      " ...\n",
      " [171 132 120 ... 147 125  39]\n",
      " [172  11  43 ...  52  84 116]\n",
      " [173 162 140 ... 119 139  60]]\n",
      "[[ 0. 25. 38. ... 39. 41. 41.]\n",
      " [ 0. 24. 39. ... 42. 42. 43.]\n",
      " [ 0. 31. 38. ... 42. 43. 43.]\n",
      " ...\n",
      " [ 0. 33. 34. ... 41. 41. 41.]\n",
      " [ 0. 33. 36. ... 41. 41. 42.]\n",
      " [ 0. 33. 38. ... 40. 41. 41.]]\n"
     ]
    }
   ],
   "source": [
    "mylsh = lsh.LSH(mentions=mentions_scaled, shingle_size=4, signature_size=300, n_buckets=2) #singhle size = 5 does not work...\n",
    "\n",
    "mylsh.cluster()\n",
    "\n",
    "\n",
    "signatures = [v[\"vector\"] for v in mylsh.mentions.values()] # use the binary vectors, not the min-hashed ones! (see wikipedia https://en.wikipedia.org/wiki/Locality-sensitive_hashing#cite_note-IndykMotwani98-5)\n",
    " # min hash is for a different approach!\n",
    "xb = np.stack([np.array(s) for s in signatures ]).astype('float32')\n",
    "\n",
    "# signature = mylsh.mentions[0][\"signature\"]\n",
    "# len(signature)\n",
    "\n",
    "d = xb.shape[1]\n",
    "nbits = 100 # is this the length of the signature?? but the signature is already in the dense vector? \n",
    "index = faiss.IndexLSH(d, nbits)   # build the index\n",
    "print(index.is_trained)\n",
    "index.add(xb)                 # add vectors to the index\n",
    "print(index.ntotal)\n",
    "\n",
    "k = 10                 \n",
    "D, I = index.search(xb, k) # sanity check -- for short nbits, it may not even assign itself as the first closest neighbor\n",
    "print(I) # the nearest neighbors change when changing the number of bits (and therefore buckets)\n",
    "print(D) # the distance here indicates whether the comparison records are in the same bucket or not\n",
    "\n",
    "\n",
    "def extract_neighbors(mentions, I):\n",
    "    neighbors = {}\n",
    "    for i, k in enumerate(mentions.values()):\n",
    "        n_idx = list(I[i])[1:] # ignore own\n",
    "        n_i = [mentions[i] for i in n_idx]\n",
    "        neighbors[k] = n_i\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "neighbors = extract_neighbors(mentions_scaled, I=I)\n",
    "neighbors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "541f9b2bd2e07a99fbe19519917a2a847d7149d43292ee48064a4037dd0b7699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
