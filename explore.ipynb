{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-sensitive hashing for detecting coreferences\n",
    "\n",
    "Problem: We want to find mentions that cold refer to the same entity. Currently we compare each mention with each other, and if mention 1 contains mention 0 as a word, then we consider mention 0 and mention 1 potential coreferences. This approach does not scale well with the number of mentions.\n",
    "\n",
    "Solution: try out how to use locality-sensitive hashing to reduce the number of comparisons to make. While there is optimized software available to do this, I think that a good start is a solution without external dependencies: no need to check compatibility with other requirements, and data are already preprocessed which should make the task computationally simple. \n",
    "\n",
    "How does LSH work?\n",
    "1. Shingling: convert text to sparse vectors. I will start with shingling at the word level and think about alternatives later.\n",
    "    - One-hot encoding\n",
    "    - Define vocabulary of all shingles\n",
    "2. MinHashing: create signatures based on randomly reordering the vocabulary and recording the first shingle that is in mention $i$.\n",
    "3. Cut the signature into bands and assign--using a hash function--all mentions to buckets. \n",
    "    - More bands means larger buckets\n",
    "    - Need to use the same function for the same band $j$ for all mentions. Can use different functions for different bands, but not required. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps, 4/1/23\n",
    "\n",
    "1. Adjust lsh to use only lists, no dict \n",
    "    - fix LSHBitSampling and LSHMinHash_nonp\n",
    "1. write tests \n",
    "    - [x] correctness of cols_to_int\n",
    "    - [ ] notebook with speed tests for different sorting options -- not for now\n",
    "    - [x] correctness of coreference classification -- precision and recall for the clustering \n",
    "    - [x] optimize for current use case (shingle size, size of bands, ...)\n",
    "2. Understand better\n",
    "    - [ ] what do shingles do? smaller ->?\n",
    "    - [ ] what does band size do? how does it interact with shingle size? does one compensate for the other? does one scale better than the other? (for optimization)\n",
    "2. Document \n",
    "    - [ ] write what the functions/classes do\n",
    "    - [ ] `cols_to_int`:  \n",
    "        - write down/explain the idea/mechanics\n",
    "        - how far can we go with until overflow error? what to do in this case?\n",
    "            - advise user to change the size of the bands (?)\n",
    "            - switch to string operation, which is (much) slower but should still work? \n",
    "3. Integrate with REL\n",
    "    - tests?\n",
    "    - compare timing with new and old approach \n",
    "4. Optimize further?\n",
    "    - alternative for minhashing? improve speed or effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_coreferences import load_coreferences\n",
    "import lsh \n",
    "import copy\n",
    "\n",
    "import faiss \n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mentions = load_coreferences(drop_duplicates=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = {i: m for i, m in enumerate(raw_mentions)}\n",
    "# stack them on top of each other \n",
    "\n",
    "mentions_scaled = copy.copy(mentions)\n",
    "\n",
    "idx = len(mentions_scaled)\n",
    "scaling_factor = 1\n",
    "for i in range(1, scaling_factor):\n",
    "    for idx_old in mentions.keys():\n",
    "        m = mentions[idx_old]\n",
    "        mentions_scaled[idx] = m \n",
    "        idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.2159130573272705 seconds for 632 mentions\n",
      "average, min, max cluster size: 17.8, 0, 49\n"
     ]
    }
   ],
   "source": [
    "len(mentions_scaled)\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=mentions_scaled, shingle_size=3, signature_size=50, band_length=2)\n",
    "\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notes for using numpy \n",
    "- what is the exact time complexity here? certainly more than linear. check the theory.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to integrate with REL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.024751663208007812 seconds for 30 mentions\n",
      "average, min, max cluster size: 8.73, 0, 19\n"
     ]
    }
   ],
   "source": [
    "# out = {i: {\"shingles\": lsh.k_shingle(m, 4)} for i, m in zip(range(len(mentions_rel), mentions)) }\n",
    "# mentions_rel\n",
    "mentions_rel = [\n",
    "    'German', 'British', 'Brussels', 'European Commission', 'German',\n",
    "    'British', 'Germany', 'European Union', 'Britain', 'Commission', \n",
    "    'European Union', 'Franz Fischler', 'Britain', 'France', 'BSE', \n",
    "    'Spanish', 'Loyola de Palacio', 'France', 'Britain', 'BSE', 'British', 'German', \n",
    "    'British', 'Europe', 'Germany', 'Bonn', 'British', 'Germany', 'Britain', 'British'\n",
    "]\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=mentions_rel, shingle_size=4, signature_size=300, band_length=2)\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSHBitSampling' object has no attribute 'mentions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b5c600994549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmylsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaiss_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmylsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmylsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# note: 50% of the time here is taken for extracting the neighbors! can this be made faster? ie with numpy?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rel20/lsh_coreferences/lsh.py\u001b[0m in \u001b[0;36mneighbors_to_dict\u001b[0;34m(self, mention_dict)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mn_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mn_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmention_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSHBitSampling' object has no attribute 'mentions'"
     ]
    }
   ],
   "source": [
    "\n",
    "mylsh = lsh.LSHBitSampling(mentions=mentions_scaled, shingle_size=4)\n",
    "\n",
    "mylsh.faiss_neighbors(k=10, nbits=100)\n",
    "neighbors = mylsh.neighbors_to_dict(mentions_scaled)\n",
    "mylsh.summarise() # note: 50% of the time here is taken for extracting the neighbors! can this be made faster? ie with numpy?\n",
    "\n",
    "# note: this approach uses quite some memory because it stores the full vectors and shingles.. change??\n",
    "# here we see the problem -- there are 20 duplicates of any mention, but the 10 nearest neighbors are all in the same bucket\n",
    "    # how to avoid? we do not know the number of duplicates beforehand... count? somehow create unique mentions to avoid this? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes \n",
    "- It is important to use the binary vectors. If using the min-hashed vectors, longer words are closer to longer ones (and shorter to shorter)\n",
    "- higher k -> higher recall\n",
    "    - low k may not capture the actual coreferences. ie, for k=4, it missed them for \"eagles\", \"rosati\", \"belo\"\n",
    "    - using k=10 fixes the problems for \"eagles\" and \"belo\" but not for \"rosati\"\n",
    "\n",
    "\n",
    "Next steps\n",
    "- scaling -- seems very good \n",
    "- precision/recall of whole pipeline (including the search for coreferences)\n",
    "- how easy is it to integrate FAISS into REL?\n",
    "- a problem could be when there are a lot of mentions that refer to the same (single) mention. ie, 10 times \"jimi\" and once \"jimi hendrix\". \n",
    "    - I think we would miss \"jimi hendrix\" for k=10\n",
    "    - Solutions: larger k? -> quadratic cost again \n",
    "    - Restrict the hamming distance of the neighbors, ie forbid them to be in the same bucket (this would also omit the own element already I think)\n",
    "    - do some more testing for this \n",
    "    - or just use the unique mentions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with MinHashing (does not scale well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 49.10193204879761 seconds for 3480 mentions\n",
      "average, min, max cluster size: 78.08, 39, 259\n"
     ]
    }
   ],
   "source": [
    "len(mentions_scaled)\n",
    "\n",
    "mylsh = lsh.LSHMinHash_nonp(mentions=mentions_scaled, shingle_size=3, signature_size=200, n_buckets=2)\n",
    "\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()\n",
    "\n",
    "# adjust the sizing according to the rules so that we have log-time complexity!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "541f9b2bd2e07a99fbe19519917a2a847d7149d43292ee48064a4037dd0b7699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
