{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-sensitive hashing for detecting coreferences\n",
    "\n",
    "Problem: We want to find mentions that cold refer to the same entity. Currently we compare each mention with each other, and if mention 1 contains mention 0 as a word, then we consider mention 0 and mention 1 potential coreferences. This approach does not scale well with the number of mentions.\n",
    "\n",
    "Solution: try out how to use locality-sensitive hashing to reduce the number of comparisons to make. While there is optimized software available to do this, I think that a good start is a solution without external dependencies: no need to check compatibility with other requirements, and data are already preprocessed which should make the task computationally simple. \n",
    "\n",
    "How does LSH work?\n",
    "1. Shingling: convert text to sparse vectors. I will start with shingling at the word level and think about alternatives later.\n",
    "    - One-hot encoding\n",
    "    - Define vocabulary of all shingles\n",
    "2. MinHashing: create signatures based on randomly reordering the vocabulary and recording the first shingle that is in mention $i$.\n",
    "3. Cut the signature into bands and assign--using a hash function--all mentions to buckets. \n",
    "    - More bands means larger buckets\n",
    "    - Need to use the same function for the same band $j$ for all mentions. Can use different functions for different bands, but not required. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps, 4/1/23\n",
    "\n",
    "1. Finish signature with numpy\n",
    "    - code below to LSH\n",
    "    - tests \n",
    "    - correctness of lsh after integration\n",
    "    - fix the (memory?) issue \n",
    "1. write tests \n",
    "    - [x] correctness of cols_to_int\n",
    "    - [ ] notebook with speed tests for different sorting options -- not for now\n",
    "    - [x] correctness of coreference classification -- precision and recall for the clustering \n",
    "    - [x] optimize for current use case (shingle size, size of bands, ...)\n",
    "2. Understand better\n",
    "    - [ ] what do shingles do? smaller ->?\n",
    "    - [ ] what does band size do? how does it interact with shingle size? does one compensate for the other? does one scale better than the other? (for optimization)\n",
    "2. Document \n",
    "    - [ ] write what the functions/classes do\n",
    "    - [ ] `cols_to_int`:  \n",
    "        - write down/explain the idea/mechanics\n",
    "        - how far can we go with until overflow error? what to do in this case?\n",
    "            - advise user to change the size of the bands (?)\n",
    "            - switch to string operation, which is (much) slower but should still work? \n",
    "3. Integrate with REL\n",
    "    - tests?\n",
    "        - how to handle empty input, input where only one item is repeated (ie \"EEC\"), ... \n",
    "    - update script for msmarco\n",
    "    - move the changed functions for signature to REL\n",
    "4. Optimize further?\n",
    "    - alternative for minhashing? improve speed or effectiveness\n",
    "    - the advantage of the while loop is the smaller memory taken. are there other ways to speed it up?\n",
    "    - or reduce signature size/shingle size for the signature to buy some speed at the expense of some correctness? how strong would the impact be?\n",
    "    - should the performance not be assessed on the universe of mentions, and not only the coreference mentions?\n",
    "    - change the band length for the performance testing?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the hashing\n",
    "The approach with min hashing is not feasible for high-dimensional data sets because it is expensive to randomly shuffle large arrays and/or store large binary arrays in memory. But there are alternatives. One is with random projections, as discussed in the wikipedia page, in [this video](https://www.youtube.com/watch?v=Arni-zkqMBA) and in [this book](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf) (section 3.7.2 Random Hyperplanes and the Cosine Distance).\n",
    "\n",
    "What is the idea?\n",
    "- Use the cosine similarity between two binary vectors with the cosine distance\n",
    "- Suppose we pick a hyperplane through the origin. We do so by choosing a vector $v$ that is perpendicular to the hyperplane. \n",
    "    - Then, either two vectors $x$ and $y$ lie on the same side of the hyperplane or they do not. \n",
    "- Then, choose a random vector $v_f$. \n",
    "    - we can build a hashing function $f$ that assigns the same value to $x$ and $y$ if they lie on the same side of the hyperplane that $v$ is perpendicular to (the dot product $x.v$ and $y.v$ will be informative about this.)\n",
    "    - (the angle $\\theta$ between $x$ and $y$ will determine the probability that $x$ and $y$ are on the same side of a given hyperplane; see book for intuition)\n",
    "    - the family $F$ of functions defined by vectors $v_f$ is locality-sensitive for the cosine distance. And this is very similar to the Jaccard-distance family, up to some scaling factor. \n",
    "- We can then build sketches (see the code [here](https://github.com/brandonrobertz/SparseLSH/blob/11f381560a94c8d74af55b3db5e8db1bbddfc212/sparselsh/lsh.py#L140)) by using random vector whose elements are either +1 or -1.\n",
    "    - consider a random vector $v$ where $v_i \\in \\{-1, 1\\} \\forall i$.\n",
    "    - we calculate the dot product for a random vector $v$ and vector $x$.\n",
    "    - the dot product is the difference between the sum of all elements $x[i]$ for $i: v[i] = 1$, and the sum of all elements $x[i]$ for $i: v[i] = -1$. \n",
    "    - We repeat this for multiple vectors $v$ and store whether the dot product is positive or negative (again by $+1$ or $-1$). Since a dot product of $0$ is unlikely, we can handle such cases arbitrarily. \n",
    "    - The result of this is called a **sketch** (which is the same as a signature, see [p. 49 here](https://web.stanford.edu/class/cs246/slides/04-lsh_theory.pdf))\n",
    "- I do not understand the example 3.22: does it imply that we have to take a large number of random vectors? what is the \n",
    "- p. 99: cosine distance makes sense in spaces [...] where points are vectors with integer components or Boolean components -- thus, we can use it here. \n",
    "\n",
    "Now I am not sure how SparseLSH implements the whole thing. They have different distance options, but the hashing is, as far as I understand, the same for all options. What is the theory behind this? Can I just use this hashing instead of my hashing, and then continue with the signature as before (which is essentially the Jaccard distance??)\n",
    "    - or is this perhaps what is discussed [here](https://ai.stackexchange.com/questions/37199/clustering-by-using-locality-sensitive-hashing-after-random-projection)?\n",
    "    - in fact, the slides from Stanford seem to imply that using cosine distance for LSH in the same way min-hashing was used for the Jaccard distance.\n",
    "\n",
    "The output from the random projections is again a (denser) vector of -1s and +1s. Because this carries much less information than the real-valued vectors from the minhashing, the banding technique does not work--too many items would have the same band. So, what does the SparseLSH apply then? What do they write in the book? What does wikipedia say?\n",
    "- see [here, p. 58](https://web.stanford.edu/class/cs246/slides/04-lsh_theory.pdf): they apply the bands technique to the Euclidean distance\n",
    "\n",
    "[continue in pdf]\n",
    "\n",
    "section 3.6: family of locality-senstive functions\n",
    "- minhash function is one family of locality-sensitive functions\n",
    "- \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, plan for 16/01\n",
    "- make toy example with dot product \n",
    "- check timing\n",
    "- should be relatively easy to integrate into REL/existing class?\n",
    "\n",
    "procedure\n",
    "1. ie, generate sparse matrix of dimension (length_signature, length(vocabulary)). one row corresponds to one hash function\n",
    "2. generate dot product of each row with the vectors (assign -1/1 depending on sign). [check again: where do they np bitpack? could this be useful here or not?]\n",
    "    - the output is one entry in the signature\n",
    "3. then use banding like I currently use. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does SparseLSH do?\n",
    "- key question: at query time, I think this transforms the keys back to sparse -- could also take a lot of memory? \n",
    "    - maybe querying one by one in REL would be an option, but that depends on how long these queries take!\n",
    "    - the `query` function seems to do quite some looping, so I am not sure how fast it is.. \n",
    "    - the advantage of the minhashing is that it returns the similarity of the items in one go (although very slowly for large inputs)\n",
    "    - can I profile their function and see if it scales well (although the output seems somewhat random at the moment)\n",
    "    - \"You do not save processing by limiting the results. Currently, a similarity ranking and sort is done on all items in the hashtable.\"\n",
    "        - where do they do the sort? \n",
    "- there must be some methods that are implemented in the `storage` class\n",
    "    - `get_list`?\n",
    "    - `append`? \n",
    "    - see below. basic dict operations suffice for the current use case. \n",
    "- at init of class:\n",
    "    - `_generate_random_planes`: \"uniformly distributed hyperplanes\" (but they use `np.random.randn`, which seems to wrap around `np.standard_normal`?)\n",
    "    - `_init_hashtables`: stores the hash tables in `storage`. if in-memory, then I think it is just a dict. \n",
    "- main steps:\n",
    "    ```python\n",
    "    lsh = SparseLSH() # init\n",
    "    lsh.index(X) # X is the sparse binary matrix\n",
    "    lsh.query()\n",
    "    ```\n",
    "- `_hash`: generate binary hashes for input points. Ie:\n",
    "    ```python\n",
    "    def _hash(self, planes, input_points): # do we do this for all hyperplanes??\n",
    "        planes = planes.transpose()\n",
    "        projections = input_points.dot(planes)\n",
    "        signs = (projections > 0) # are these the sketches?\n",
    "        return  np.packbits(signs.toarray(), axis=-1) #  Packs the elements of a binary-valued array into bits in a uint8 array.\n",
    "    ```\n",
    "- `index()` function\n",
    "    ```python\n",
    "    def index(self, input_points, extra_data=None): # s\n",
    "        \" Index input points by adding them to the selected storage.\"\n",
    "        # assume extra_data is always `None`.\n",
    "        for i, table in enumerate(self.hash_tables):\n",
    "            keys = self._hash(self.uniform_planes[i])\n",
    "\n",
    "            for j in range(keys.shape[0]): # not sure what this does\n",
    "                value = (input_points[j],)\n",
    "                table.append_val(keys[j].tobytes(), value) # what does append_val do? I guess it adds the data to the storage. in the present case, just use memory \n",
    "    ```\n",
    "- `query()` function\n",
    "    - for a distance function, return the closests candidates\n",
    "    - matches exactly on one of the hash keys \n",
    "    ```python\n",
    "    candidates = []\n",
    "    for i, table in enumerate(self.hash_tables):\n",
    "        # get hashes of query points for the specific plane\n",
    "        keys = self._hash(self.uniform_planes[i], query_points)\n",
    "        for j in range(keys.shape[0]):\n",
    "            # Create a sublist of candidate neighbors for each query point\n",
    "            if len(candidates) <= j:\n",
    "                candidates.append([])\n",
    "            new_candidates = table.get_list(keys[j].tobytes())\n",
    "            if new_candidates is not None and len(new_candidates) > 0:\n",
    "                candidates[j].extend(new_candidates) # what does extend do? \n",
    "        # some parts are skipped .. \n",
    "        distances = d_func(query_points[j], cand_csr) # main function. how big is cand_csr? this could be very expensive if cand_csr is large \n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the class for in-memory storage. these are just a bunch of operations on dictionaries. how exactly are the data stored?\n",
    "\n",
    "```python\n",
    "class InMemoryStorage(BaseStorage):\n",
    "    def __init__(self, config):\n",
    "        self.name = 'dict'\n",
    "        self.storage = dict()\n",
    "\n",
    "    def keys(self):\n",
    "        return list(self.storage.keys())\n",
    "\n",
    "    def set_val(self, key, val):\n",
    "        self.storage[key] = val\n",
    "\n",
    "    def get_val(self, key):\n",
    "        return self.storage[key]\n",
    "\n",
    "    def append_val(self, key, val):\n",
    "        self.storage.setdefault(key, []).append(val) # what does setdefault do? \n",
    "\n",
    "    def get_list(self, key):\n",
    "        return self.storage.get(key, [])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([216], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([5,6,-1,6,9,-11])\n",
    "signs = (a > 0)\n",
    "signs\n",
    "\n",
    "np.packbits(signs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_coreferences import load_coreferences\n",
    "# import lsh \n",
    "import REL # install with pip install -e ../REL/.\n",
    "import copy\n",
    "\n",
    "import faiss \n",
    "import numpy as np\n",
    "from REL import lsh \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mentions = load_coreferences(drop_duplicates=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = {i: m for i, m in enumerate(raw_mentions)}\n",
    "# stack them on top of each other \n",
    "\n",
    "mentions_scaled = copy.copy(mentions)\n",
    "\n",
    "idx = len(mentions_scaled)\n",
    "scaling_factor = 5\n",
    "for i in range(1, scaling_factor):\n",
    "    for idx_old in mentions.keys():\n",
    "        m = mentions[idx_old]\n",
    "        mentions_scaled[idx] = m \n",
    "        idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.1436002254486084 seconds for 3160 mentions\n",
      "average, min, max cluster size: 187.92, 14, 379\n"
     ]
    }
   ],
   "source": [
    "len(mentions_scaled)\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=mentions_scaled, shingle_size=2, signature_size=200, band_length=10)\n",
    "\n",
    "# mylsh.cluster(numpy_signature=True)\n",
    "# mylsh.summarise()\n",
    "\n",
    "mylsh.cluster(numpy_signature=False)\n",
    "mylsh.summarise()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notes for using numpy \n",
    "- what is the exact time complexity here? certainly more than linear. check the theory.\n",
    "- (probably) memory issues when making the signature with numpy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to integrate with REL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.0069942474365234375 seconds for 30 mentions\n",
      "average, min, max cluster size: 29.0, 29, 29\n"
     ]
    }
   ],
   "source": [
    "# out = {i: {\"shingles\": lsh.k_shingle(m, 4)} for i, m in zip(range(len(mentions_rel), mentions)) }\n",
    "# mentions_rel\n",
    "mentions_rel = [\n",
    "    'German', 'British', 'Brussels', 'European Commission', 'German',\n",
    "    'British', 'Germany', 'European Union', 'Britain', 'Commission', \n",
    "    'European Union', 'Franz Fischler', 'Britain', 'France', 'BSE', \n",
    "    'Spanish', 'Loyola de Palacio', 'France', 'Britain', 'BSE', 'British', 'German', \n",
    "    'British', 'Europe', 'Germany', 'Bonn', 'British', 'Germany', 'Britain', 'British'\n",
    "]\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=mentions_rel, shingle_size=4, signature_size=50, band_length=2)\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to deal with input where only one item is repeated? -- put as a test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1}, {0, 1}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lsh \n",
    "import numpy as np\n",
    "\n",
    "test_mentions = ['EEC', 'EEC'] # this fails. ['German', 'German'] does not. neither does ['EEC', 'EEC', 'German']\n",
    "test_mentions = ['EEC', 'ABC'] # this also fails. ['EEC', 'ABCde'] does not. \n",
    "# the problem is that when there is no mention of at least the inputted shingle size, the binary vectors cannot be calculated\n",
    "\n",
    "mylsh = lsh.LSHMinHash(mentions=test_mentions, shingle_size=4, signature_size=300, band_length=2)\n",
    "\n",
    "mylsh._build_vocab()\n",
    "mylsh.encode_binary(to_numpy=True)\n",
    "\n",
    "mylsh.vectors\n",
    "\n",
    "mylsh.vectors.shape[1] # this should not be 0\n",
    "mylsh.vectors.shape\n",
    "# mylsh.make_signature()\n",
    "mylsh.cluster()\n",
    "mylsh.candidates\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.34625840187072754 seconds to classify 3160 mentions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mylsh = lsh.LSHBitSampling(mentions=mentions_scaled, shingle_size=4)\n",
    "\n",
    "mylsh.faiss_neighbors(k=10, nbits=100)\n",
    "neighbors = mylsh.neighbors_to_dict()\n",
    "mylsh.summarise() # note: 50% of the time here is taken for extracting the neighbors! can this be made faster? ie with numpy?\n",
    "\n",
    "\n",
    "\n",
    "# note: this approach uses quite some memory because it stores the full vectors and shingles.. change??\n",
    "# here we see the problem -- there are 20 duplicates of any mention, but the 10 nearest neighbors are all in the same bucket\n",
    "    # how to avoid? we do not know the number of duplicates beforehand... count? somehow create unique mentions to avoid this? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes \n",
    "- It is important to use the binary vectors. If using the min-hashed vectors, longer words are closer to longer ones (and shorter to shorter)\n",
    "- higher k -> higher recall\n",
    "    - low k may not capture the actual coreferences. ie, for k=4, it missed them for \"eagles\", \"rosati\", \"belo\"\n",
    "    - using k=10 fixes the problems for \"eagles\" and \"belo\" but not for \"rosati\"\n",
    "\n",
    "\n",
    "Next steps\n",
    "- scaling -- seems very good \n",
    "- precision/recall of whole pipeline (including the search for coreferences)\n",
    "- how easy is it to integrate FAISS into REL?\n",
    "- a problem could be when there are a lot of mentions that refer to the same (single) mention. ie, 10 times \"jimi\" and once \"jimi hendrix\". \n",
    "    - I think we would miss \"jimi hendrix\" for k=10\n",
    "    - Solutions: larger k? -> quadratic cost again \n",
    "    - Restrict the hamming distance of the neighbors, ie forbid them to be in the same bucket (this would also omit the own element already I think)\n",
    "    - do some more testing for this \n",
    "    - or just use the unique mentions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new approach with random projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "\n",
    "raw_mentions\n",
    "mylsh = lsh.LSHMinHash(mentions=raw_mentions, shingle_size=4, signature_size=50, band_length=2)\n",
    "\n",
    "# mylsh._build_vocab()\n",
    "# mylsh.encode_binary()\n",
    "# vectors = sparse.csr_matrix(mylsh.vectors)\n",
    "\n",
    "# mylsh.make_signature()\n",
    "\n",
    "mylsh.cluster()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 880)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "length_signature = 40\n",
    "rng = np.random.default_rng(seed=3)\n",
    "vectors = mylsh.vectors\n",
    "hyperplanes = rng.choice([-1, 1], (length_signature, vectors.shape[1]))\n",
    "hyperplanes = sparse.csr_matrix(hyperplanes)\n",
    "\n",
    "hyperplanes[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 880)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(632, 880)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(880, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 1, 2, 2],\n",
       "       [1, 2, 2, ..., 2, 2, 1],\n",
       "       [2, 2, 2, ..., 1, 2, 2],\n",
       "       ...,\n",
       "       [2, 1, 2, ..., 2, 1, 2],\n",
       "       [2, 1, 2, ..., 1, 1, 2],\n",
       "       [2, 1, 2, ..., 2, 1, 2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plane = hyperplanes[1, :]\n",
    "display(plane.shape)\n",
    "display(vectors.shape)\n",
    "display(plane.transpose().shape)\n",
    "out = vectors.dot(plane.transpose())\n",
    "out = out.toarray()\n",
    "\n",
    "sig_i = (out > 0)\n",
    "\n",
    "sig_i = sig_i.astype(int)\n",
    "\n",
    "1 + sig_i\n",
    "\n",
    "test = vectors.dot(hyperplanes.transpose()) #.shape\n",
    "test = test.toarray()\n",
    "test = (test > 0)\n",
    "1 + test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with MinHashing (does not scale well, and not work at the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 49.10193204879761 seconds for 3480 mentions\n",
      "average, min, max cluster size: 78.08, 39, 259\n"
     ]
    }
   ],
   "source": [
    "len(mentions_scaled)\n",
    "\n",
    "mylsh = lsh.LSHMinHash_nonp(mentions=mentions_scaled, shingle_size=3, signature_size=200, n_buckets=2)\n",
    "\n",
    "mylsh.cluster()\n",
    "mylsh.summarise()\n",
    "\n",
    "# adjust the sizing according to the rules so that we have log-time complexity!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create signature with numpy -- main function to put to lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_rows_reps(a):\n",
    "    \"reshape a 3-d array of n_reps x n_rows x n_cols to n_rows x n_reps x n_cols\"\n",
    "    n_reps, n_rows, n_cols = a.shape\n",
    "    a = a.reshape(n_reps*n_rows, n_cols)\n",
    "    # extractor indices: for 3 reps, 2 rows: [0,2,4,1,3,5]. to reorder a\n",
    "        # in other words: goes from 0 to (n_reps * n_rows). step sizes are n_rows. starts are the row indices\n",
    "    idx = np.arange(n_reps*n_rows).reshape(n_reps, n_rows).T.reshape(-1,1)\n",
    "    a = np.take_along_axis(a, idx, axis=0)\n",
    "    a = a.reshape(n_rows, n_reps, n_cols)\n",
    "    return a \n",
    "\n",
    "def minhash_signature_np(x, n_reps):\n",
    "    \"\"\"Make a minhash signature of array x with length n_reps.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: axis 0 are observations, columns are binary one-hot encoded vectors\n",
    "    \"\"\"\n",
    "    # get indices \n",
    "    indices = np.arange(x.shape[1])\n",
    "    rng = np.random.default_rng(12345)\n",
    "\n",
    "    # expand by n_reps \n",
    "    indices_mult = np.tile(indices, (n_reps, 1)) # reorder the columns n_reps times \n",
    "    x_mult = np.tile(x, (n_reps, 1)).reshape((n_reps,) + x.shape) # new shape: (n_resp, x.shape[0], x.shape[1\n",
    "\n",
    "    # permute indices and apply to x_mult\n",
    "    permuted_indices = rng.permuted(indices_mult, axis=1)\n",
    "    x_mult_permuted = np.take_along_axis(x_mult, permuted_indices[:, np.newaxis], 2)\n",
    "\n",
    "    # for the reduction below, need to have all samples of the same observation in one block\n",
    "    x_mult_permuted = reshape_rows_reps(x_mult_permuted)\n",
    "\n",
    "    # make signature\n",
    "    sig = x_mult_permuted.argmax(axis=2)\n",
    "    return sig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42382504,  1.26372846, -0.87066174, -0.25917323],\n",
       "       [-0.07534331, -0.74088465, -1.3677927 ,  0.6488928 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## simulate input \n",
    "n_cols = 4 # (size of dict?)\n",
    "n_rows = 2 # number of rows in original array (mnumber of mentions)\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.standard_normal(size=n_rows*n_cols).reshape(n_rows, n_cols)\n",
    "# x = rng.choice(range(2), size=n_rows*n_cols, p=[0.9, 0.1]).reshape(n_rows, n_cols)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 1],\n",
       "       [2, 2, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "minhash_signature_np(x, 3)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of links for this re-sorting\n",
    "- https://stackoverflow.com/questions/35646908/numpy-shuffle-multidimensional-array-by-row-only-keep-column-order-unchanged\n",
    "- https://stackoverflow.com/questions/70683286/fastest-way-to-sample-many-random-permutations-of-a-numpy-array\n",
    "- https://stackoverflow.com/questions/20265229/rearrange-columns-of-numpy-2d-array\n",
    "- https://stackoverflow.com/questions/53421637/reorganizing-a-3d-numpy-array\n",
    "- https://stackoverflow.com/questions/64235838/how-to-sort-each-row-of-a-3d-numpy-array-by-another-2d-array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to order a 3-dimensional array along one axis? -- put to general AAR\n",
    "\n",
    "- important: the input array and the ordering array need to have the same dimensions when using `np.take_along_axis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [10 20 30 40]\n",
      "x reordered: [40 10 20 30]\n",
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10, 20, 30, 40],\n",
       "       [10, 20, 30, 40]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x reordered:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[40, 10, 20, 30],\n",
       "       [40, 10, 20, 30]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40]],\n",
       "\n",
       "       [[10, 20, 30, 40]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x reordered:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[40, 10, 20, 30]],\n",
       "\n",
       "       [[40, 10, 20, 30]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]],\n",
       "\n",
       "       [[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x reordered:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[40, 10, 20, 30],\n",
       "        [ 3,  5,  8,  9]],\n",
       "\n",
       "       [[30, 10, 40, 20],\n",
       "        [ 9,  5,  3,  8]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one dimension\n",
    "x = np.array([10, 20, 30, 40])\n",
    "print(f\"x: {x}\")\n",
    "p = np.array([3,0,1,2])\n",
    "print(f\"x reordered: {x[p]}\")\n",
    "# two dimensions\n",
    "x = np.array([[10, 20, 30, 40],[10, 20, 30, 40]] )\n",
    "print(f\"x:\")\n",
    "display(x)\n",
    "print(f\"x reordered:\")\n",
    "display(x[:, p])\n",
    "# three dimensions\n",
    "x = np.array([[10, 20, 30, 40],[10, 20, 30, 40]])[:, np.newaxis]\n",
    "print(\"x:\")\n",
    "display(x)\n",
    "print(\"x reordered:\")\n",
    "display(x[:, :, p])\n",
    "# display(x)\n",
    "\n",
    "# # now add another dimension to p \n",
    "p2 = np.array([[3,0,1,2], [2,0,3,1]])\n",
    "display(p2[:, np.newaxis].shape)\n",
    "display(x.shape)\n",
    "np.take_along_axis(x, p2[:, np.newaxis], 2)\n",
    "\n",
    "# now add expand dimension 1 of x \n",
    "x = np.array([[[10, 20, 30, 40], [5,8,9,3]],[[10, 20, 30, 40], [5,8,9,3]]] )\n",
    "print(\"x:\")\n",
    "display(x)\n",
    "print(\"x reordered:\")\n",
    "np.take_along_axis(x, p2[:, np.newaxis], 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another application: reorder the rows -- how is that different from the above? -- Put to AAR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40]],\n",
       "\n",
       "       [[ 8,  4,  8,  1]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reordered x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 8,  4,  8,  1]],\n",
       "\n",
       "       [[10, 20, 30, 40]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]],\n",
       "\n",
       "       [[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [1]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [10, 20, 30, 40]],\n",
       "\n",
       "       [[ 5,  8,  9,  3],\n",
       "        [ 5,  8,  9,  3]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start again with a simple example\n",
    "x = np.array([[1,2],[3,4]])\n",
    "idx = np.array([1, 0])\n",
    "x[idx, :]\n",
    "\n",
    "## three dimensions\n",
    "x = np.array([[10, 20, 30, 40],[8, 4, 8, 1]])[:, np.newaxis]\n",
    "print(\"x:\")\n",
    "display(x)\n",
    "print(\"reordered x:\")\n",
    "display(x[idx, :])\n",
    "\n",
    "\n",
    "## three dimensions, multiple rows\n",
    "x = np.array([\n",
    "    [[10, 20, 30, 40], [5,8,9,3]], # [0, 0, 0, 0]],\n",
    "    [[10, 20, 30, 40], [5,8,9,3]]#, [0, 0, 0, 0]] # this also does not work \n",
    "    ] )\n",
    "print(\"x:\")\n",
    "display(x)\n",
    "\n",
    "reordered_idx = np.array([[0, 0], [1, 1]]).reshape(2,2,1)\n",
    "display(reordered_idx)\n",
    "\n",
    "np.take_along_axis(x, reordered_idx, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]],\n",
       "\n",
       "       [[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]],\n",
       "\n",
       "       [[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reordered x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [10, 20, 30, 40],\n",
       "        [10, 20, 30, 40]],\n",
       "\n",
       "       [[ 5,  8,  9,  3],\n",
       "        [ 5,  8,  9,  3],\n",
       "        [ 5,  8,  9,  3]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# more reps -- TODO: test case 1\n",
    "x = np.array([ \n",
    "    [[10, 20, 30, 40], [5,8,9,3]],\n",
    "    [[10, 20, 30, 40], [5,8,9,3]],\n",
    "    [[10, 20, 30, 40], [5,8,9,3]]\n",
    "    ])\n",
    "print(\"x:\")\n",
    "display(x)\n",
    "\n",
    "print(\"reordered x:\")\n",
    "display(reshape_rows_reps(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3],\n",
       "        [ 0,  0,  0,  0]],\n",
       "\n",
       "       [[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3],\n",
       "        [ 0,  0,  0,  0]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reordered x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [10, 20, 30, 40]],\n",
       "\n",
       "       [[ 5,  8,  9,  3],\n",
       "        [ 5,  8,  9,  3]],\n",
       "\n",
       "       [[ 0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# more rows -- TODO: test case 2\n",
    "x = np.array([\n",
    "    [[10, 20, 30, 40], [5,8,9,3], [0, 0, 0, 0]],\n",
    "    [[10, 20, 30, 40], [5,8,9,3], [0, 0, 0, 0]] # this also does not work \n",
    "    ] )\n",
    "print(\"x:\")\n",
    "display(x)\n",
    "\n",
    "print(\"reordered x:\")\n",
    "display(reshape_rows_reps(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.take_along_axis.html\n",
    "Ni, M, Nk = a.shape[:axis], a.shape[axis], a.shape[axis+1:]\n",
    "# let a be an array with n_reps of n_rows rows with n_cols cols. a.shape = (n_reps, n_rows, n_cols)\n",
    "# Ni: length of all dimensions until axis. in my case, [n_reps]\n",
    "# M: n_rows \n",
    "# Nk: n_cols\n",
    "J = indices.shape[axis]  # Need not equal M\n",
    "# J: n_reps\n",
    "out = np.empty(Ni + (J,) + Nk) \n",
    "\n",
    "for ii in ndindex(Ni):\n",
    "    for kk in ndindex(Nk):\n",
    "        a_1d       = a      [ii + s_[:,] + kk]\n",
    "        indices_1d = indices[ii + s_[:,] + kk]\n",
    "        out_1d     = out    [ii + s_[:,] + kk]\n",
    "        for j in range(J):\n",
    "            out_1d[j] = a_1d[indices_1d[j]]\n",
    "# output: Ni x J x Nk = n_reps x n_reps x n_cols. but what I want is n_rows x n_reps x n_cols (?) thus, I need to change the input, ie to change the shape of a.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3],\n",
       "        [ 0,  0,  0,  0]],\n",
       "\n",
       "       [[10, 20, 30, 40],\n",
       "        [ 5,  8,  9,  3],\n",
       "        [ 0,  0,  0,  0]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[10, 20, 30, 40],\n",
       "        [ 0,  0,  0,  0],\n",
       "        [ 5,  8,  9,  3]],\n",
       "\n",
       "       [[ 5,  8,  9,  3],\n",
       "        [10, 20, 30, 40],\n",
       "        [ 0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(x)\n",
    "xb = x.reshape(3,8) # 3 = number of reps, 8 = n_col * n_rows\n",
    "out = np.stack(np.split(xb,2,axis=1)) # 2 = n_rows\n",
    "out # this works, but is it efficient?\n",
    "# https://stackoverflow.com/questions/21580693/numpy-array-split-partition-efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw multiple samples of a given array where the columns are randomly reordered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3, 10],\n",
       "       [ 4,  5,  6, 20]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 1, 2, 3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 1, 2],\n",
       "       [1, 2, 3, 0],\n",
       "       [1, 3, 0, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1, 10,  2,  3],\n",
       "        [ 4, 20,  5,  6]],\n",
       "\n",
       "       [[ 2,  3, 10,  1],\n",
       "        [ 5,  6, 20,  4]],\n",
       "\n",
       "       [[ 2, 10,  1,  3],\n",
       "        [ 5, 20,  4,  6]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "x = np.array([[1,2,3, 10], [4,5,6,20]]) # the starting array \n",
    "display(x)\n",
    "\n",
    "indices = np.arange(x.shape[1])\n",
    "n_reps = 3\n",
    "indices_mult = np.tile(indices, (n_reps, 1)) # reorder the columns n_reps times \n",
    "x_mult = np.tile(x, (n_reps, 1)).reshape((n_reps,) + x.shape) # new shape: (n_resp, x.shape[0], x.shape[1\n",
    "\n",
    "display(indices_mult)\n",
    "# permute \n",
    "permuted_indices = rng.permuted(indices_mult, axis=1)\n",
    "display(permuted_indices)\n",
    "\n",
    "# reorder the sample\n",
    "np.take_along_axis(x_mult, permuted_indices[:, np.newaxis], 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permuting columns: compare approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.7 ms ± 318 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "23.6 ms ± 394 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.64 ms ± 24.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rng = np.random.default_rng(1234)\n",
    "# x is big enough to not want to enumerate all permutations\n",
    "x = rng.standard_normal(size=20)\n",
    "n = 10000\n",
    "perms = np.array([rng.permutation(x) for _ in range(n)])\n",
    "perms \n",
    "\n",
    "def list_comp(x, n):\n",
    "    rng_temp = np.random.default_rng(1)\n",
    "    perms = np.array([rng_temp.permutation(x) for _ in range(n)])\n",
    "    return perms \n",
    "\n",
    "\n",
    "def while_loop(x, n):\n",
    "    rng_temp = np.random.default_rng(1)\n",
    "    i = 0 \n",
    "    templist = []\n",
    "    while i < n:\n",
    "        templist.append(rng_temp.permutation(x))\n",
    "        i += 1\n",
    "    return np.stack(templist) \n",
    "\n",
    "def np_permuted(x, n):\n",
    "    rng_temp = np.random.default_rng(1)\n",
    "    perms = rng_temp.permuted(np.tile(x, n).reshape(n,x.size), axis=1)\n",
    "    return perms \n",
    "\n",
    "out1 = list_comp(x, n)\n",
    "out2 = while_loop(x, n)\n",
    "out3 = np_permuted(x, n)\n",
    "\n",
    "assert not np.any(out1 != out2)\n",
    "assert not np.any(out1 != out3)\n",
    "\n",
    "%timeit list_comp(x, n)\n",
    "%timeit while_loop(x, n)\n",
    "%timeit np_permuted(x, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "541f9b2bd2e07a99fbe19519917a2a847d7149d43292ee48064a4037dd0b7699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
